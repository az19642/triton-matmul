{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zv8fsa3uE_Hy",
        "outputId": "7b99e276-2abc-4391-d866-56aa694cd824"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nvidia-cutlass\n",
            "  Downloading nvidia_cutlass-3.6.0.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: cuda-python>=11.8.0 in /usr/local/lib/python3.11/dist-packages (from nvidia-cutlass) (12.2.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from nvidia-cutlass) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from nvidia-cutlass) (1.26.4)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.11/dist-packages (from nvidia-cutlass) (3.0.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from nvidia-cutlass) (1.13.1)\n",
            "Collecting treelib (from nvidia-cutlass)\n",
            "  Downloading treelib-1.7.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from cuda-python>=11.8.0->nvidia-cutlass) (3.0.11)\n",
            "Requirement already satisfied: pyparsing>=3.0.9 in /usr/local/lib/python3.11/dist-packages (from pydot->nvidia-cutlass) (3.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from treelib->nvidia-cutlass) (1.17.0)\n",
            "Downloading nvidia_cutlass-3.6.0.0-py3-none-any.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading treelib-1.7.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: treelib, nvidia-cutlass\n",
            "Successfully installed nvidia-cutlass-3.6.0.0 treelib-1.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install nvidia-cutlass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA2fKqnquoIw",
        "outputId": "c307a7cd-4025-4737-98ea-d0cd9c6bb9e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Jan 21 00:07:30 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HcGBvyJl6vzf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import cutlass\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UXlZHAElvh4F"
      },
      "outputs": [],
      "source": [
        "@triton.jit\n",
        "def base_kernel(\n",
        "    a_ptr,\n",
        "    b_ptr,\n",
        "    c_ptr,\n",
        "    M,\n",
        "    N,\n",
        "    K,\n",
        "    stride_am,\n",
        "    stride_ak,\n",
        "    stride_bk,\n",
        "    stride_bn,\n",
        "    stride_cm,\n",
        "    stride_cn,\n",
        "    GROUP_SIZE_M: tl.constexpr,\n",
        "    BLOCK_SIZE_M: tl.constexpr,\n",
        "    BLOCK_SIZE_N: tl.constexpr,\n",
        "    BLOCK_SIZE_K: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Triton tutorial implementation of the kernel for computing the matmul C = A x B.\n",
        "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
        "    \"\"\"\n",
        "    pid = tl.program_id(axis=0)\n",
        "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
        "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
        "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
        "    group_id = pid // num_pid_in_group\n",
        "    first_pid_m = group_id * GROUP_SIZE_M\n",
        "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
        "    pid_m = first_pid_m + (pid % group_size_m)\n",
        "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
        "\n",
        "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
        "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
        "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
        "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
        "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
        "\n",
        "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
        "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
        "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
        "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
        "        accumulator = tl.dot(a, b, accumulator)\n",
        "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
        "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
        "    c = accumulator.to(tl.float16)\n",
        "\n",
        "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
        "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
        "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
        "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
        "    tl.store(c_ptrs, c, mask=c_mask)\n",
        "\n",
        "\n",
        "def matmul(a, b, kernel, GROUP_SIZE_M):\n",
        "    \"\"\"\n",
        "    Perform matrix multiplication using the provided matmul kernel.\n",
        "\n",
        "    This is identical to the tutorial implementation, except that we allow for the option of passing in kernel\n",
        "    parameters directly for later plotting (passing in GROUP_SIZE_M) purposes.\n",
        "    \"\"\"\n",
        "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
        "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
        "\n",
        "    M, K = a.shape\n",
        "    K, N = b.shape\n",
        "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
        "    grid = lambda META: (triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"]),)\n",
        "    kernel[grid](\n",
        "        a,\n",
        "        b,\n",
        "        c,\n",
        "        M,\n",
        "        N,\n",
        "        K,\n",
        "        a.stride(0),\n",
        "        a.stride(1),\n",
        "        b.stride(0),\n",
        "        b.stride(1),\n",
        "        c.stride(0),\n",
        "        c.stride(1),\n",
        "        GROUP_SIZE_M=GROUP_SIZE_M,\n",
        "    )\n",
        "\n",
        "    return c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jMJtD3ojBZqv"
      },
      "outputs": [],
      "source": [
        "def run_benchmarks(block_size_lst, gsm_lst, num_stages_lst, num_warps_lst) -> None:\n",
        "    \"\"\"\n",
        "    Plot the performance of the matmul kernel autotuned at every different (GROUP_SIZE_M, K).\n",
        "    \"\"\"\n",
        "    configs = []\n",
        "    for bsm in block_size_lst:\n",
        "        for bsn in block_size_lst:\n",
        "            for bsk in block_size_lst:\n",
        "                for ns in num_stages_lst:\n",
        "                    for nw in num_warps_lst:\n",
        "                        configs.append(\n",
        "                            triton.Config(\n",
        "                                {\n",
        "                                    \"BLOCK_SIZE_M\": bsm,\n",
        "                                    \"BLOCK_SIZE_N\": bsn,\n",
        "                                    \"BLOCK_SIZE_K\": bsk,\n",
        "                                },\n",
        "                                num_stages=ns,\n",
        "                                num_warps=nw,\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "    tunable_kernel = triton.autotune(configs=[configs], key=[\"K\", \"GROUP_SIZE_M\"])(base_kernel)\n",
        "\n",
        "    benches = [\n",
        "        triton.testing.Benchmark(\n",
        "            x_names=[\"K\"],\n",
        "            x_vals=[i for i in range(1024, 8193, 1024)],\n",
        "            line_arg=\"provider\",\n",
        "            line_vals=[\"triton\", \"cublas\", \"cutlass\"],\n",
        "            line_names=[\"Triton\", \"cuBLAS\", \"cuTLASS\"],\n",
        "            styles=[(\"red\", \"-\"), (\"green\", \"-\"), (\"blue\", \"-\")],\n",
        "            ylabel=\"Time (ms)\",\n",
        "            plot_name=f\"gsm{gsm}_per-k-triton-autotuned_matmul_row-major_fp16\",\n",
        "            args={\n",
        "                \"M\": 8192,\n",
        "                \"N\": 8192,\n",
        "                \"gsm\": gsm,\n",
        "            },\n",
        "        )\n",
        "        for gsm in gsm_lst\n",
        "    ]\n",
        "\n",
        "    @triton.testing.perf_report(benches)\n",
        "    def benchmark(M, N, K, gsm, provider):\n",
        "        a = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n",
        "        b = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "        plan = cutlass.op.Gemm(element=torch.float16, layout=cutlass.LayoutType.RowMajor)\n",
        "        c = torch.ones((M, N), device=\"cuda\", dtype=torch.float16)\n",
        "        d = torch.ones((M, N), device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "        if provider == \"triton\":\n",
        "            mean_ms = triton.testing.do_bench(lambda: matmul(a, b, tunable_kernel, gsm))\n",
        "        elif provider == \"cublas\":\n",
        "            mean_ms = triton.testing.do_bench(lambda: torch.matmul(a, b))\n",
        "        elif provider == \"cutlass\":\n",
        "            mean_ms = triton.testing.do_bench(lambda: plan.run(a, b, c, d))\n",
        "\n",
        "        return mean_ms\n",
        "\n",
        "    benchmark.run(print_data=True, show_plots=True, save_path=\"./per-k-gsm-triton-autotuned_matmul_perf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gtkbO8mBbNo"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TRITON_PRINT_AUTOTUNING\"] = \"1\"\n",
        "os.environ[\"MLIR_ENABLE_DUMP\"] = \"1\"\n",
        "os.environ[\"TRITON_ALWAYS_COMPILE\"] = \"1\"\n",
        "os.environ[\"LLVM_IR_ENABLE_DUMP\"] = \"1\"\n",
        "\n",
        "# Lists of values for each parameter to grid tune over\n",
        "block_size_lst = [64]\n",
        "num_stages_lst = [2]\n",
        "num_warps_lst = [8]\n",
        "gsm_list = [1, 4, 8]\n",
        "\n",
        "with open(\"autotuning_output\", \"w\") as sys.stdout:\n",
        "    run_benchmarks(block_size_lst, gsm_list, num_stages_lst, num_warps_lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU-SljPj9q3T",
        "outputId": "e9153c4e-20f3-4e6f-9184-5f628724d783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/per-k-autotuned_matmul_perf/ (stored 0%)\n",
            "  adding: content/per-k-autotuned_matmul_perf/per-k-autotuned_matmul_row-major_fp16.csv (deflated 50%)\n",
            "  adding: content/per-k-autotuned_matmul_perf/per-k-autotuned_matmul_row-major_fp16.png (deflated 90%)\n",
            "  adding: content/per-k-autotuned_matmul_perf/results.html (deflated 6%)\n",
            "  adding: content/autotuned_matmul_perf/ (stored 0%)\n",
            "  adding: content/autotuned_matmul_perf/GSM12_autotuned_matmul_row-major_fp16.csv (deflated 52%)\n",
            "  adding: content/autotuned_matmul_perf/GSM12_autotuned_matmul_row-major_fp16.png (deflated 90%)\n",
            "  adding: content/autotuned_matmul_perf/GSM10_autotuned_matmul_row-major_fp16.png (deflated 90%)\n",
            "  adding: content/autotuned_matmul_perf/GSM2_autotuned_matmul_row-major_fp16.csv (deflated 53%)\n",
            "  adding: content/autotuned_matmul_perf/GSM14_autotuned_matmul_row-major_fp16.png (deflated 90%)\n",
            "  adding: content/autotuned_matmul_perf/GSM6_autotuned_matmul_row-major_fp16.png (deflated 90%)\n",
            "  adding: content/autotuned_matmul_perf/GSM4_autotuned_matmul_row-major_fp16.png (deflated 90%)\n",
            "  adding: content/autotuned_matmul_perf/results.html (deflated 76%)\n",
            "  adding: content/autotuned_matmul_perf/GSM2_autotuned_matmul_row-major_fp16.png (deflated 90%)\n",
            "  adding: content/autotuned_matmul_perf/GSM8_autotuned_matmul_row-major_fp16.csv (deflated 52%)\n",
            "  adding: content/autotuned_matmul_perf/GSM8_autotuned_matmul_row-major_fp16.png (deflated 90%)\n",
            "  adding: content/autotuned_matmul_perf/GSM6_autotuned_matmul_row-major_fp16.csv (deflated 53%)\n",
            "  adding: content/autotuned_matmul_perf/GSM10_autotuned_matmul_row-major_fp16.csv (deflated 53%)\n",
            "  adding: content/autotuned_matmul_perf/GSM14_autotuned_matmul_row-major_fp16.csv (deflated 53%)\n",
            "  adding: content/autotuned_matmul_perf/GSM4_autotuned_matmul_row-major_fp16.csv (deflated 53%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /content/per-k-gsm-triton-autotuned_matmul_perf.zip /content/per-k-gsm-triton-autotuned_matmul_perf/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Zc88L4Ri9ty_",
        "outputId": "94d9c410-65fc-49d7-b557-e4ea968e28ee"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_060f0915-50a9-490e-b457-efc5fd6aca02\", \"per-k-autotuned_matmul_perf.zip\", 1671)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_accba886-e668-4a8e-83da-a7e9560855f2\", \"autotuned_matmul_perf.zip\", 10406)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/per-k-gsm-triton-autotuned_matmul_perf.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
